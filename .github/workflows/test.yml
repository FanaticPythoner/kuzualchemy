# SPDX-FileCopyrightText: 2025 FanaticPythoner
# SPDX-License-Identifier: Apache-2.0

name: Run Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_call:
    outputs:
      test-passed:
        description: "Number of tests that passed"
        value: ${{ jobs.collect-test-results.outputs.test-passed }}
      test-failed:
        description: "Number of tests that failed"
        value: ${{ jobs.collect-test-results.outputs.test-failed }}
      test-error:
        description: "Number of tests with errors"
        value: ${{ jobs.collect-test-results.outputs.test-error }}
      test-total:
        description: "Total number of tests"
        value: ${{ jobs.collect-test-results.outputs.test-total }}
      test-summary:
        description: "Test summary string"
        value: ${{ jobs.collect-test-results.outputs.test-summary }}

jobs:
  test:
    runs-on: ubuntu-latest
    # Skip if commit is from auto-release workflow to avoid unnecessary test runs
    if: "!contains(github.event.head_commit.message, 'chore: update README.md with version') && !contains(github.event.head_commit.message, '[auto-release-skip]')"
    strategy:
      matrix:
        # || S.S. Python versions ORDERED from OLDEST to NEWEST
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12', '3.13']

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install -e ".[test]"

    - name: Install pytest-json-report
      run: |
        python -m pip install pytest-json-report

    - name: Run tests with pytest
      id: pytest
      run: |
        if [ "${{ matrix.python-version }}" = "3.11" ]; then
          # Run tests and capture results (only for Python 3.11 to avoid duplicate outputs)
          # EXCLUDE stress tests to prevent interference
          python -m pytest tests/ -v --tb=short --json-report --json-report-file=test-results.json --ignore=tests/test_stress_and_reliability.py > pytest_output.txt 2>&1 || true

          # Parse test results
          if [ -f test-results.json ]; then
            # Extract test counts using Python one-liner
            PASSED=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('passed', 0))")
            FAILED=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('failed', 0))")
            ERROR=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('error', 0))")
            TOTAL=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('total', 0))")

            echo "PYTEST_PASSED=$PASSED"
            echo "PYTEST_FAILED=$FAILED"
            echo "PYTEST_ERROR=$ERROR"
            echo "PYTEST_TOTAL=$TOTAL"

            echo "passed=$PASSED" >> $GITHUB_OUTPUT
            echo "failed=$FAILED" >> $GITHUB_OUTPUT
            echo "error=$ERROR" >> $GITHUB_OUTPUT
            echo "total=$TOTAL" >> $GITHUB_OUTPUT

            # Extract the pytest summary line
            if [ -f pytest_output.txt ]; then
              # Find the summary line with passed and timing info, then clean it
              SUMMARY_LINE=$(grep -E "=+ .* passed.* in .* =+" pytest_output.txt | tail -1)
              if [ -n "$SUMMARY_LINE" ]; then
                # Extract content between = signs and remove warnings
                SUMMARY=$(echo "$SUMMARY_LINE" | sed 's/^=*[[:space:]]*//' | sed 's/[[:space:]]*=*$//' | sed 's/, [0-9]* warnings//')
                echo "Test summary: $SUMMARY"
                echo "summary=$SUMMARY" >> $GITHUB_OUTPUT
              else
                echo "summary=$PASSED passed" >> $GITHUB_OUTPUT
              fi
            else
              echo "summary=$PASSED passed" >> $GITHUB_OUTPUT
            fi
          else
            echo "No test results file found"
            echo "passed=0" >> $GITHUB_OUTPUT
            echo "failed=0" >> $GITHUB_OUTPUT
            echo "error=1" >> $GITHUB_OUTPUT
            echo "total=1" >> $GITHUB_OUTPUT
            echo "summary=Tests failed" >> $GITHUB_OUTPUT
          fi

          # Clean up
          rm -f test-results.json pytest_output.txt
        else
          # For other Python versions, just run tests normally (EXCLUDE stress tests)
          python -m pytest tests/ -v --tb=short --ignore=tests/test_stress_and_reliability.py
        fi

    - name: Run tests with coverage
      if: matrix.python-version == '3.11'
      run: |
        python -m pip install pytest-cov
        # EXCLUDE stress tests from coverage to prevent interference
        python -m pytest tests/ -v --tb=short --cov=src/kuzualchemy --cov-report=xml --cov-report=term --ignore=tests/test_stress_and_reliability.py

  # Separate job to collect test results and provide outputs
  collect-test-results:
    runs-on: ubuntu-latest
    needs: test
    if: "always() && !contains(github.event.head_commit.message, 'chore: update README.md with version') && !contains(github.event.head_commit.message, '[auto-release-skip]')"
    outputs:
      test-passed: ${{ steps.results.outputs.passed }}
      test-failed: ${{ steps.results.outputs.failed }}
      test-error: ${{ steps.results.outputs.error }}
      test-total: ${{ steps.results.outputs.total }}
      test-summary: ${{ steps.results.outputs.summary }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install -e ".[test]"
        python -m pip install pytest-json-report

    - name: Create test result parser script
      run: |
        echo 'import json, sys, os' > parse_results.py
        echo 'if not os.path.exists("test-results.json"): print("ERROR: test-results.json not found", file=sys.stderr); sys.exit(1)' >> parse_results.py
        echo 'try:' >> parse_results.py
        echo '    with open("test-results.json", "r") as f: data = json.load(f)' >> parse_results.py
        echo 'except Exception as e: print(f"ERROR parsing JSON: {e}", file=sys.stderr); sys.exit(1)' >> parse_results.py
        echo 'summary = data.get("summary", {})' >> parse_results.py
        echo 'if not summary: print("ERROR: No summary section in JSON", file=sys.stderr); sys.exit(1)' >> parse_results.py
        echo 'print("PASSED=" + str(summary.get("passed", 0)))' >> parse_results.py
        echo 'print("FAILED=" + str(summary.get("failed", 0)))' >> parse_results.py
        echo 'print("ERROR=" + str(summary.get("error", 0)))' >> parse_results.py
        echo 'print("TOTAL=" + str(summary.get("total", 0)))' >> parse_results.py

    - name: Run tests and collect results
      id: results
      run: |
        # Run tests and capture results (continue even if tests fail)
        # EXCLUDE stress tests from collection to prevent interference
        python -m pytest tests/ -v --tb=short --json-report --json-report-file=test-results.json --ignore=tests/test_stress_and_reliability.py > pytest_output.txt 2>&1 || true
        TEST_EXIT_CODE=$?

        echo "=== PYTEST EXIT CODE: $TEST_EXIT_CODE ==="
        echo "=== PYTEST OUTPUT FILE EXISTS: $(test -f pytest_output.txt && echo 'YES' || echo 'NO') ==="
        echo "=== JSON RESULTS FILE EXISTS: $(test -f test-results.json && echo 'YES' || echo 'NO') ==="

        # Show the last few lines of pytest output for debugging
        echo "=== LAST 10 LINES OF PYTEST OUTPUT ==="
        tail -10 pytest_output.txt || echo "Could not read pytest output"

        # Parse test results with robust error handling and fallbacks
        if [ ! -f test-results.json ]; then
          echo "WARNING: test-results.json not found, using fallback values"
          PASSED=0
          FAILED=0
          ERROR=1
          TOTAL=1
          SUMMARY="Tests failed - no results file"
        else
          # Extract test counts using the parser script
          RESULT_OUTPUT=$(python parse_results.py 2>/dev/null)
          if [ $? -ne 0 ]; then
            echo "WARNING: Failed to parse test results, using fallback values"
            PASSED=0
            FAILED=0
            ERROR=1
            TOTAL=1
            SUMMARY="Tests failed - parsing error"
          else
            # Parse the output
            eval "$RESULT_OUTPUT"

            # Validate extracted values
            if [ -z "$PASSED" ]; then PASSED=0; fi
            if [ -z "$FAILED" ]; then FAILED=0; fi
            if [ -z "$ERROR" ]; then ERROR=0; fi
            if [ -z "$TOTAL" ]; then TOTAL=0; fi
          fi
        fi

        echo "=== EXTRACTED TEST COUNTS ==="
        echo "PASSED: $PASSED"
        echo "FAILED: $FAILED"
        echo "ERROR: $ERROR"
        echo "TOTAL: $TOTAL"

        # Extract the pytest summary line with fallbacks
        if [ ! -f pytest_output.txt ]; then
          echo "WARNING: pytest_output.txt not found, using fallback summary"
          if [ -z "$SUMMARY" ]; then
            SUMMARY="Tests completed (output unavailable)"
          fi
        else
          # Find the summary line with passed and timing info
          SUMMARY_LINE=$(grep -E "=+ .* passed.* in .* =+" pytest_output.txt | tail -1)
          if [ -z "$SUMMARY_LINE" ]; then
            echo "WARNING: Could not find pytest summary line, checking for other patterns"
            # Try alternative patterns
            SUMMARY_LINE=$(grep -E "=+ .* failed.* in .* =+" pytest_output.txt | tail -1)
            if [ -z "$SUMMARY_LINE" ]; then
              SUMMARY_LINE=$(grep -E "=+ .* in .* =+" pytest_output.txt | tail -1)
            fi
          fi

          if [ -n "$SUMMARY_LINE" ]; then
            # Extract content between = signs and remove warnings
            SUMMARY=$(echo "$SUMMARY_LINE" | sed 's/^=*[[:space:]]*//' | sed 's/[[:space:]]*=*$//' | sed 's/, [0-9]* warnings//')
          else
            echo "WARNING: No summary line found, generating from counts"
            if [ "$FAILED" -gt 0 ] || [ "$ERROR" -gt 0 ]; then
              SUMMARY="$FAILED failed, $ERROR errors"
            elif [ "$PASSED" -gt 0 ]; then
              SUMMARY="$PASSED passed"
            else
              SUMMARY="Tests completed (details unavailable)"
            fi
          fi
        fi

        echo "=== EXTRACTED SUMMARY ==="
        echo "SUMMARY_LINE: $SUMMARY_LINE"
        echo "SUMMARY: $SUMMARY"

        # Ensure we have valid values before setting outputs
        if [ -z "$PASSED" ]; then PASSED=0; fi
        if [ -z "$FAILED" ]; then FAILED=0; fi
        if [ -z "$ERROR" ]; then ERROR=0; fi
        if [ -z "$TOTAL" ]; then TOTAL=0; fi
        if [ -z "$SUMMARY" ]; then SUMMARY="Tests completed (details unavailable)"; fi

        echo "=== FINAL VALUES BEFORE OUTPUT ==="
        echo "PASSED='$PASSED' FAILED='$FAILED' ERROR='$ERROR' TOTAL='$TOTAL' SUMMARY='$SUMMARY'"

        echo "passed=$PASSED" >> $GITHUB_OUTPUT
        echo "failed=$FAILED" >> $GITHUB_OUTPUT
        echo "error=$ERROR" >> $GITHUB_OUTPUT
        echo "total=$TOTAL" >> $GITHUB_OUTPUT
        echo "summary=$SUMMARY" >> $GITHUB_OUTPUT

        echo "=== OUTPUTS SET SUCCESSFULLY ==="

  # Isolated stress test job - runs independently to avoid database interference
  stress-tests:
    runs-on: ubuntu-latest
    # Skip if commit is from auto-release workflow to avoid unnecessary test runs
    if: "!contains(github.event.head_commit.message, 'chore: update README.md with version') && !contains(github.event.head_commit.message, '[auto-release-skip]')"

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install -e ".[test]"

    - name: Run stress tests in isolation
      id: stress_tests
      run: |
        echo "Running stress and reliability tests in complete isolation..."

        # Create a unique temporary directory for this test run
        TEMP_DB_DIR=$(mktemp -d)
        export KUZU_TEST_DB_PATH="$TEMP_DB_DIR/stress_test.db"

        echo "Using isolated database: $KUZU_TEST_DB_PATH"

        # Run ONLY the stress tests with verbose output
        python -m pytest tests/test_stress_and_reliability.py -v --tb=long -s --no-header

        # Clean up
        rm -rf "$TEMP_DB_DIR"

        echo "Stress tests completed successfully"

    - name: Stress test summary
      if: always()
      run: |
        if [ "${{ steps.stress_tests.outcome }}" = "success" ]; then
          echo "✅ Stress and reliability tests PASSED"
        else
          echo "❌ Stress and reliability tests FAILED"
          echo "Check the logs above for detailed failure information"
        fi
